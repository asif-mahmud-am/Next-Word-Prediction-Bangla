{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:21.538927Z",
     "iopub.status.busy": "2022-02-01T08:29:21.538272Z",
     "iopub.status.idle": "2022-02-01T08:29:21.563393Z",
     "shell.execute_reply": "2022-02-01T08:29:21.562728Z",
     "shell.execute_reply.started": "2022-02-01T08:29:21.538837Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import io\n",
    "import codecs\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:25.312354Z",
     "iopub.status.busy": "2022-02-01T08:29:25.311687Z",
     "iopub.status.idle": "2022-02-01T08:29:25.316878Z",
     "shell.execute_reply": "2022-02-01T08:29:25.316143Z",
     "shell.execute_reply.started": "2022-02-01T08:29:25.312306Z"
    }
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with io.open(filename, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:27.772003Z",
     "iopub.status.busy": "2022-02-01T08:29:27.771496Z",
     "iopub.status.idle": "2022-02-01T08:29:27.781193Z",
     "shell.execute_reply": "2022-02-01T08:29:27.780464Z",
     "shell.execute_reply.started": "2022-02-01T08:29:27.771966Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean corpus\n",
    "def clean_doc(doc):\n",
    "   \n",
    "    doc = doc.replace('?', ' ? ')\n",
    "    doc = doc.replace('।', ' । ')\n",
    "    doc = doc.replace(',', ' । ')\n",
    "    doc = doc.replace('-', ' । ')\n",
    "    doc = doc.replace('(', ' । ')\n",
    "    doc = doc.replace(')', ' । ')\n",
    "    doc = doc.replace('{', ' । ')\n",
    "    doc = doc.replace('}', ' । ')\n",
    "    doc = doc.replace('[', ' । ')\n",
    "    doc = doc.replace(']', ' । ')\n",
    "    doc = doc.replace('*', ' ')\n",
    "    doc = doc.replace('^', ' ')\n",
    "    doc = doc.replace('#', ' ')\n",
    "    doc = doc.replace('~', ' ')\n",
    "    doc = doc.replace('`', ' ')\n",
    "    doc = doc.replace('/', ' ')\n",
    "    doc = doc.replace('_', ' ')\n",
    "    doc = doc.replace('’', ' ')\n",
    "    doc = doc.replace('‘', ' ')\n",
    "    doc = doc.replace('.',' । ')\n",
    "    \n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:32.025233Z",
     "iopub.status.busy": "2022-02-01T08:29:32.024519Z",
     "iopub.status.idle": "2022-02-01T08:29:32.030424Z",
     "shell.execute_reply": "2022-02-01T08:29:32.029654Z",
     "shell.execute_reply.started": "2022-02-01T08:29:32.025186Z"
    }
   },
   "outputs": [],
   "source": [
    "# save tokens to file\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w', encoding='utf8')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:35.902236Z",
     "iopub.status.busy": "2022-02-01T08:29:35.901557Z",
     "iopub.status.idle": "2022-02-01T08:29:35.937579Z",
     "shell.execute_reply": "2022-02-01T08:29:35.936849Z",
     "shell.execute_reply.started": "2022-02-01T08:29:35.902195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "সিজিপিএ ভালো থাকা লাগবে । এই স্কলারশিপের মজা হলো ও\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = r'../../dataset/original dataset.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:39.982739Z",
     "iopub.status.busy": "2022-02-01T08:29:39.9818Z",
     "iopub.status.idle": "2022-02-01T08:29:40.026709Z",
     "shell.execute_reply": "2022-02-01T08:29:40.025954Z",
     "shell.execute_reply.started": "2022-02-01T08:29:39.982699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['সিজিপিএ', 'ভালো', 'থাকা', 'লাগবে', '।', 'এই', 'স্কলারশিপের', 'মজা', 'হলো', 'ওরা', 'আপনাকে', 'ইয়োরোপের', 'দুটো', 'দেশের', 'দুই', 'বিশ্ববিদ্যালয়ে', 'যৌথভাবে', 'দুই', 'বছরের', 'মাস্টার্স', 'ডিগ্রি', 'করাবে', '।', 'বৃত্তির', 'পরিমাণ', 'বেশ', 'ভালো', '।', 'কোনো', 'চাকরির', 'অভিজ্ঞতা', 'লাগবেনা', '।', 'বিশ্বের', 'যেকোনো', 'বিশ্ববিদ্যালয়', 'থেকে', 'আপনি', 'আনকন্ডিশনাল', 'অফার', 'লেটার', 'পেলে', 'আপনি', 'এটায়', 'এপ্লাই', 'করতে', 'পারবেন', '।', 'আইল্টস', 'দেওয়া', 'লাগবে', '।', 'বিশ্ববিদ্যালয়', 'অফার', 'পাওয়ার', 'পর', 'আলদাভাবে', 'এপ্লাই', 'করতে', 'হবে', '।', 'সাধারণত', 'জানুয়ারি', 'ফেব্রুয়ারিতে', 'ওপেন', 'হয়ে', 'এপ্রিল', 'পর্যন্ত', 'সময়', 'থাকে', '।', 'বিশ্বের', 'ভালো', 'র\\u200d্যাংকের', 'বিশ্ববিদ্যালয়ে', 'ভর্তি', 'হয়ে', 'অফার', 'লেটার', 'নিয়ে', 'এই', 'স্কলারশিপে', 'এপ্লাই', 'করতে', 'হয়', '।', 'সাধারণত', 'উন্নয়ন', 'বিষয়ক', 'পড়ালেখার', 'জন্য', 'এই', 'বৃত্তি', 'দেওয়া', 'হয়', '।', 'দারুণ', 'একটা', 'সিভি', 'লাগবে']\n",
      "Total Tokens: 77589\n",
      "Unique Tokens: 14116\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:100])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:29:52.500232Z",
     "iopub.status.busy": "2022-02-01T08:29:52.499519Z",
     "iopub.status.idle": "2022-02-01T08:29:52.557312Z",
     "shell.execute_reply": "2022-02-01T08:29:52.556545Z",
     "shell.execute_reply.started": "2022-02-01T08:29:52.500196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77587\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 1 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T08:30:07.046717Z",
     "iopub.status.busy": "2022-02-01T08:30:07.046214Z",
     "iopub.status.idle": "2022-02-01T08:30:07.062451Z",
     "shell.execute_reply": "2022-02-01T08:30:07.061586Z",
     "shell.execute_reply.started": "2022-02-01T08:30:07.046679Z"
    }
   },
   "outputs": [],
   "source": [
    "# save unigram sequences to file\n",
    "out_filename = r'../../dataset/unigram_tokenized_dataset.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77586\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 2 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bigram sequences to file\n",
    "out_filename = r'../../dataset/bigram_tokenized_dataset.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77585\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 3 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trigram sequences to file\n",
    "out_filename = r'../../dataset/trigram_tokenized_dataset.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77584\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 4 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 4gram sequences to file\n",
    "out_filename = r'../../dataset/4gram_tokenized_dataset.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 77583\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 5 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 5gram sequences to file\n",
    "out_filename = r'../../dataset/5gram_tokenized_dataset.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "90ddd76d85c6d8f9c7c37ce266baa8c2cb75316408a94f8d07998045da23b63c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
